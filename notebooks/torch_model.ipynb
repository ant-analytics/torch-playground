{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This machine run on cuda with 8 cpus\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "# set global state\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_cpu = os.cpu_count()\n",
    "print(f'This machine run on {device} with {num_cpu} cpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "# load config\n",
    "config = load_config('/mnt/d/torch-playground/scripts/config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "    Custom dataset class for loading\n",
    "    input: file path\n",
    "    output: tensor\n",
    "    '''\n",
    "    def __init__(self, file_path, target_status, target_score):\n",
    "        self.file_path = file_path\n",
    "        self.target_status = target_status\n",
    "        self.target_score = target_score\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "        self.X = self.data.drop([target_status, target_score], axis=1)\n",
    "        self.y_status = self.data[target_status]\n",
    "        self.y_score = self.data[target_score]\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        return the number of samples\n",
    "        '''\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        return the sample at index idx\n",
    "        if use .reshape(-1, 1) to add dimension --> every sample will be shape (1,1)\n",
    "        when batched, it will be shape (batch_size, 1, 1)\n",
    "        if use .unsqueeze(-1) to add dimension --> every sample will be shape (1,)\n",
    "        when batched, it will be shape (batch_size, 1); use .unsqueeze(-1) recommended\n",
    "        '''\n",
    "        \n",
    "        X = torch.tensor(self.X.iloc[idx].values, dtype=torch.float32)\n",
    "        target_status = torch.tensor(self.y_status.iloc[idx], dtype=torch.float32).unsqueeze(-1) # add dimension\n",
    "        target_score = torch.tensor(self.y_score.iloc[idx], dtype=torch.float32).unsqueeze(-1) # add dimension\n",
    "        return X, target_score, target_status\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "# data \n",
    "train_path = config['data']['train_path']\n",
    "val_path = config['data']['val_path']\n",
    "test_path = config['data']['test_path']\n",
    "target_status = config['data']['target_status']\n",
    "target_score = config['data']['target_score']\n",
    "batch_size = config['training']['batch_size']\n",
    "\n",
    "# model\n",
    "input_size = config['model']['input_size']\n",
    "hidden_size_1 = config['model']['hidden_size_1']\n",
    "hidden_size_2 = config['model']['hidden_size_2']\n",
    "hidden_size_3 = config['model']['hidden_size_3']\n",
    "output_size_status = config['model']['output_size_status']\n",
    "output_size_score = config['model']['output_size_score']\n",
    "\n",
    "# training\n",
    "learning_rate = config['training']['learning_rate']\n",
    "num_epochs = config['training']['num_epochs']\n",
    "\n",
    "# logging\n",
    "timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "checkpoint_dir = config['logging']['checkpoint_dir']\n",
    "\n",
    "tensorboard_log_dir = f'{config['logging']['tensorboard_log_dir']}/{timestamp}'\n",
    "writer = SummaryWriter(log_dir=tensorboard_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset and train loader\n",
    "train_dataset = CustomDataset(file_path=train_path, target_status=target_status, target_score=target_score)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_cpu)\n",
    "\n",
    "# validation dataset and validation loader\n",
    "val_dataset = CustomDataset(file_path=val_path, target_status=target_status, target_score=target_score)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cpu)\n",
    "\n",
    "# test dataset and test loader\n",
    "test_dataset = CustomDataset(test_path, target_status=target_status, target_score=target_score)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is shape of X: torch.Size([64, 18])\n",
      "this is shape of y_status: torch.Size([64, 1])\n",
      "this is shape of y_score: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y_status, y_score) in enumerate(train_loader):\n",
    "    print(f'this is shape of X: {X.shape}')\n",
    "    print(f'this is shape of y_status: {y_status.shape}')\n",
    "    print(f'this is shape of y_score: {y_score.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "class LoanModel(nn.Module):\n",
    "    '''\n",
    "    Docstring for LoanModel\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, output_size_score, output_size_status):\n",
    "        '''\n",
    "        Docstring for __init__\n",
    "        '''\n",
    "        super().__init__() # no need to pass class and self\n",
    "\n",
    "        # shared layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n",
    "        \n",
    "        # output layer for regression\n",
    "        self.regression_head = nn.Linear(hidden_size_3, output_size_score) # output size for regression\n",
    "\n",
    "        # output layer for binary classification\n",
    "        self.classification_head = nn.Linear(hidden_size_3, output_size_status) # output size for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Docstring for forward\n",
    "        '''\n",
    "        # shared layers\n",
    "        # relu activation function for hidden layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "\n",
    "        # regression head\n",
    "        score_output = self.regression_head(x) # no activation function for regression\n",
    "\n",
    "        # classification head\n",
    "        # sigmoid for binary classification\n",
    "        status_output = torch.sigmoid(self.classification_head(x))\n",
    "\n",
    "        return score_output, status_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss for credit score\n",
    "score_loss_fn = nn.MSELoss()\n",
    "status_loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  instantiate model\n",
    "model = LoanModel(input_size, hidden_size_1, hidden_size_2, hidden_size_3, output_size_score, output_size_status).to(device)\n",
    "# score optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoanModel(\n",
       "  (fc1): Linear(in_features=18, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (regression_head): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (classification_head): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, score_loss_fn, status_loss_fn, device):\n",
    "    '''\n",
    "    Docstring for evaluate_model\n",
    "    :param model: Description\n",
    "    :type model: \n",
    "    :param val_loader: Description\n",
    "    :type val_loader: \n",
    "    :param criterion: Description\n",
    "    :type criterion: \n",
    "    :param device: Description\n",
    "    :type device: '''\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, target_score, target_status in val_loader:\n",
    "            X, target_score, target_status = X.to(device), target_status.to(device), target_score.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            score_output, status_output = model(X)\n",
    "\n",
    "            # calculate score loss\n",
    "            score_loss_val = score_loss_fn(score_output, target_score)\n",
    "\n",
    "            # calculate status loss\n",
    "            status_loss_val = status_loss_fn(status_output, target_status)\n",
    "\n",
    "            # total loss\n",
    "            total_loss = score_loss_val + status_loss_val\n",
    "\n",
    "            # accumulate loss\n",
    "            running_val_loss += total_loss.item()\n",
    "\n",
    "    avg_running_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    # return average validation loss\n",
    "    return score_loss_val, status_loss_val, avg_running_val_loss\n",
    "\n",
    "\n",
    "\n",
    "# training loop\n",
    "def train_model(model, train_loader, val_loader, score_loss_fn, status_loss_fn, optimiser, device, num_epochs):\n",
    "    '''\n",
    "    docstring for _train_model\n",
    "    '''\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # set best validation loss to infinity\n",
    "    best_val_loss = np.inf\n",
    "    epoch_no_improve = 0  # counting epochs with no improvement for early stopping\n",
    "    patience = 5  # number of epochs to wait before stopping training\n",
    "    final_epoch = 0\n",
    "\n",
    "    # loop through num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        # loop through the training data\n",
    "        for X, target_score, target_status in train_loader:\n",
    "            X, target_score, target_status = X.to(device), target_score.to(device), target_status.to(device)\n",
    "\n",
    "            # zero the gradients\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            score_output, status_output = model(X)\n",
    "\n",
    "            # calculate trainng loss for score\n",
    "            score_loss_training = score_loss_fn(score_output, target_score)\n",
    "\n",
    "            # calculate training loss for status\n",
    "            status_loss_training = status_loss_fn(status_output, target_status)\n",
    "\n",
    "            # total training loss\n",
    "            total_loss_training = score_loss_training + status_loss_training\n",
    "\n",
    "            # accumulate training loss\n",
    "            running_train_loss += total_loss_training.item()\n",
    "\n",
    "        # average training loss\n",
    "        avg_running_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "        # evaluate model\n",
    "        score_loss_val, status_loss_val, avg_running_val_loss = evaluate_model(\n",
    "            model=model,\n",
    "            val_loader=val_loader, \n",
    "            score_loss_fn=score_loss_fn, \n",
    "            status_loss_fn=status_loss_fn, \n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # log to training loss for score and status for each epoch\n",
    "        writer.add_scalar('score_loss/Training', score_loss_training, epoch)\n",
    "        writer.add_scalar('status_loss/Training', status_loss_training, epoch)\n",
    "\n",
    "        # log to validation loss for score and status for each epoch\n",
    "        writer.add_scalar('score_loss/Validation', score_loss_val, epoch)\n",
    "        writer.add_scalar('status_loss/Validation', status_loss_val, epoch)\n",
    "\n",
    "        # log average training and validation loss for each epoch\n",
    "        writer.add_scalar('Average_Loss/Training', avg_running_train_loss, epoch)\n",
    "        writer.add_scalar('Average_Loss/Validation', avg_running_val_loss, epoch)\n",
    "\n",
    "        # print training and validation loss for each epoch\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs} | Average Training Loss: {avg_running_train_loss:.4f} | Average Validation Loss: {avg_running_val_loss:.4f}')\n",
    "\n",
    "        # backward pass\n",
    "        total_loss_training.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimiser.step()\n",
    "\n",
    "        # save model checkpoint if validation loss improves\n",
    "        if avg_running_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_running_val_loss\n",
    "            torch.save(model.state_dict(), f'{checkpoint_dir}/loan_score_best_model_{timestamp}.pth')\n",
    "            epoch_no_improve = 0\n",
    "            print(f'Model improved, saving at epoch: {epoch+1} with validation loss: {best_val_loss:.4f}. Checkpoint saved.')\n",
    "        else:\n",
    "            epoch_no_improve += 1\n",
    "            print(f'No improvement, patience: {epoch_no_improve}/{patience}')\n",
    "\n",
    "        if epoch_no_improve >= patience:\n",
    "            print('Early stopping triggered.')\n",
    "            print(f'stop at epoch: {epoch+1}')\n",
    "            final_epoch += epoch + 1\n",
    "            break\n",
    "\n",
    "    print('Training complete.')\n",
    "    return final_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of X is torch.Size([64, 18])\n",
      "the shape of y_1 is torch.Size([64, 1])\n",
      "the shape of y_2 is torch.Size([64, 1])\n",
      "-------------------\n",
      "the shape of X is torch.Size([64, 18])\n",
      "the shape of y_1 is torch.Size([64, 1])\n",
      "the shape of y_2 is torch.Size([64, 1])\n",
      "-------------------\n",
      "the shape of X is torch.Size([64, 18])\n",
      "the shape of y_1 is torch.Size([64, 1])\n",
      "the shape of y_2 is torch.Size([64, 1])\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y_1, y_2) in enumerate(train_loader):\n",
    "    print(f'the shape of X is {X.shape}')\n",
    "    print(f'the shape of y_1 is {y_1.shape}')\n",
    "    print(f'the shape of y_2 is {y_2.shape}')\n",
    "    print('-------------------')\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 | Average Training Loss: 1.1469 | Average Validation Loss: 0.9907\n",
      "Model improved, saving at epoch: 1 with validation loss: 0.9907. Checkpoint saved.\n",
      "Epoch: 2/100 | Average Training Loss: 1.1059 | Average Validation Loss: 0.9729\n",
      "Model improved, saving at epoch: 2 with validation loss: 0.9729. Checkpoint saved.\n",
      "Epoch: 3/100 | Average Training Loss: 1.0691 | Average Validation Loss: 0.9576\n",
      "Model improved, saving at epoch: 3 with validation loss: 0.9576. Checkpoint saved.\n",
      "Epoch: 4/100 | Average Training Loss: 1.0352 | Average Validation Loss: 0.9444\n",
      "Model improved, saving at epoch: 4 with validation loss: 0.9444. Checkpoint saved.\n",
      "Epoch: 5/100 | Average Training Loss: 1.0036 | Average Validation Loss: 0.9329\n",
      "Model improved, saving at epoch: 5 with validation loss: 0.9329. Checkpoint saved.\n",
      "Epoch: 6/100 | Average Training Loss: 0.9744 | Average Validation Loss: 0.9229\n",
      "Model improved, saving at epoch: 6 with validation loss: 0.9229. Checkpoint saved.\n",
      "Epoch: 7/100 | Average Training Loss: 0.9474 | Average Validation Loss: 0.9143\n",
      "Model improved, saving at epoch: 7 with validation loss: 0.9143. Checkpoint saved.\n",
      "Epoch: 8/100 | Average Training Loss: 0.9212 | Average Validation Loss: 0.9066\n",
      "Model improved, saving at epoch: 8 with validation loss: 0.9066. Checkpoint saved.\n",
      "Epoch: 9/100 | Average Training Loss: 0.8949 | Average Validation Loss: 0.8995\n",
      "Model improved, saving at epoch: 9 with validation loss: 0.8995. Checkpoint saved.\n",
      "Epoch: 10/100 | Average Training Loss: 0.8681 | Average Validation Loss: 0.8932\n",
      "Model improved, saving at epoch: 10 with validation loss: 0.8932. Checkpoint saved.\n",
      "Epoch: 11/100 | Average Training Loss: 0.8404 | Average Validation Loss: 0.8880\n",
      "Model improved, saving at epoch: 11 with validation loss: 0.8880. Checkpoint saved.\n",
      "Epoch: 12/100 | Average Training Loss: 0.8120 | Average Validation Loss: 0.8842\n",
      "Model improved, saving at epoch: 12 with validation loss: 0.8842. Checkpoint saved.\n",
      "Epoch: 13/100 | Average Training Loss: 0.7835 | Average Validation Loss: 0.8827\n",
      "Model improved, saving at epoch: 13 with validation loss: 0.8827. Checkpoint saved.\n",
      "Epoch: 14/100 | Average Training Loss: 0.7555 | Average Validation Loss: 0.8839\n",
      "No improvement, patience: 1/5\n",
      "Epoch: 15/100 | Average Training Loss: 0.7285 | Average Validation Loss: 0.8886\n",
      "No improvement, patience: 2/5\n",
      "Epoch: 16/100 | Average Training Loss: 0.7035 | Average Validation Loss: 0.8975\n",
      "No improvement, patience: 3/5\n",
      "Epoch: 17/100 | Average Training Loss: 0.6810 | Average Validation Loss: 0.9114\n",
      "No improvement, patience: 4/5\n",
      "Epoch: 18/100 | Average Training Loss: 0.6620 | Average Validation Loss: 0.9313\n",
      "No improvement, patience: 5/5\n",
      "Early stopping triggered.\n",
      "stop at epoch: 18\n",
      "Training complete.\n",
      "training time: 48.20703983306885\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "final_epoch = train_model(model=model, train_loader=train_loader, val_loader=val_loader, num_epochs=num_epochs, score_loss_fn=score_loss_fn, status_loss_fn=status_loss_fn, optimiser=optimizer, device=device)\n",
    "end = time.time()\n",
    "print(f'training time: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint\n",
    "# Automatically find the latest checkpoint based on the timestamp in the filename\n",
    "import glob\n",
    "import os\n",
    "\n",
    "checkpoint_dir = config['logging']['checkpoint_dir']\n",
    "latest_checkpoint = max(glob.glob(os.path.join(checkpoint_dir, 'loan_score_best_model_*.pth')), key=os.path.getctime)\n",
    "\n",
    "# Load the latest model checkpoint\n",
    "model.load_state_dict(torch.load(latest_checkpoint))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_score_loss, test_status_loss, test_avg_loss = evaluate_model(\n",
    "    model=model,\n",
    "    val_loader=test_loader,\n",
    "    score_loss_fn=score_loss_fn,\n",
    "    status_loss_fn=status_loss_fn,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# # log test loss\n",
    "# writer.add_scalar('score_loss/Test', test_score_loss)\n",
    "# writer.add_scalar('status_loss/Test', test_status_loss)\n",
    "# writer.add_scalar('Average_Loss/Test', test_avg_loss)\n",
    "writer.add_scalar('Average_Loss/Training', test_avg_loss, global_step=final_epoch)\n",
    "writer.add_scalar('Average_Loss/Validation', test_avg_loss, global_step=final_epoch)\n",
    "\n",
    "# Flush  and close the writer\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
